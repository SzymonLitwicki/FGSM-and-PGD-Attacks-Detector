# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C3_RzJbeeFtRE_BqPMfJtoCvItbjQ5jX
"""

# ============================================================================
# CELL 1: Installation and Setup
# ============================================================================

!pip install -q adversarial-robustness-toolbox

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import os
import json
import shutil
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions

from art.attacks.evasion import ProjectedGradientDescent
from art.estimators.classification import TensorFlowV2Classifier

import pandas as pd

print("‚úÖ All packages loaded!")
print(f"   TensorFlow version: {tf.__version__}")
print(f"   GPU available: {tf.config.list_physical_devices('GPU')}")

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# ============================================================================
# CELL 2: Configuration
# ============================================================================

class Config:
    # Dataset
    DATASET_PATH = '/root/.cache/kagglehub/datasets/prasunroy/natural-images/versions/1/natural_images'

    # All classes
    ALL_CLASSES = ['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']

    # Load ALL images
    IMAGES_PER_CLASS = None
    IMG_SIZE = 224

    # PGD Attack Parameters
    PGD_CONFIGS = [
           {'eps': 0.10, 'eps_step': 0.005, 'max_iter': 40},
           {'eps': 0.01, 'eps_step': 0.001, 'max_iter': 10},
           {'eps': 0.03, 'eps_step': 0.003, 'max_iter': 15},
           {'eps': 0.04, 'eps_step': 0.004, 'max_iter': 15},
    ]

    # Working directories
    BASE_DIR = '/content/adversarial_pgd_dataset'
    ADVERSARIAL_DIR = os.path.join(BASE_DIR, 'adversarial_images')
    RESULTS_DIR = os.path.join(BASE_DIR, 'results')
    PLOTS_DIR = os.path.join(BASE_DIR, 'plots')

    RANDOM_SEED = 42

config = Config()

# Create directories
for directory in [config.BASE_DIR, config.ADVERSARIAL_DIR, config.RESULTS_DIR, config.PLOTS_DIR]:
    os.makedirs(directory, exist_ok=True)

print("‚öôÔ∏è  Configuration:")
print(f"   Dataset: Natural Images (FULL)")
print(f"   Classes: {config.ALL_CLASSES}")
print(f"   Attack: PGD (Projected Gradient Descent)")
print(f"   Configurations:")
for cfg in config.PGD_CONFIGS:
    print(f"      Œµ={cfg['eps']}, step={cfg['eps_step']}, iter={cfg['max_iter']}")
print(f"   Output directory: {config.BASE_DIR}")
print("\n‚úÖ Directories created!")

# ============================================================================
# CELL 3: Download and Load FULL Natural Images Dataset
# ============================================================================

import kagglehub

print("üì• Downloading Natural Images dataset...")
dataset_path = kagglehub.dataset_download("prasunroy/natural-images")

if os.path.exists(os.path.join(dataset_path, 'natural_images')):
    config.DATASET_PATH = os.path.join(dataset_path, 'natural_images')
else:
    config.DATASET_PATH = dataset_path

print(f"‚úÖ Dataset path: {config.DATASET_PATH}")

# ============================================================================
# Load ALL images from dataset
# ============================================================================

print("\n" + "="*70)
print("üìä LOADING FULL DATASET")
print("="*70)

all_image_paths = []  # Store paths, not arrays (saves memory)

for class_idx, class_name in enumerate(config.ALL_CLASSES):
    class_path = os.path.join(config.DATASET_PATH, class_name)

    if not os.path.exists(class_path):
        print(f"   ‚ùå Class not found: {class_name}")
        continue

    # Get ALL image files
    all_files = [f for f in os.listdir(class_path)
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    print(f"   üìÇ {class_name}: Found {len(all_files)} images")

    for img_file in all_files:
        all_image_paths.append({
            'class': class_name,
            'class_idx': class_idx,
            'filename': img_file,
            'path': os.path.join(class_path, img_file)
        })

print(f"\n‚úÖ Dataset indexed!")
print(f"   Total images: {len(all_image_paths)}")

# Show distribution
print(f"\nüìä Class distribution:")
for class_name in config.ALL_CLASSES:
    count = sum(1 for img in all_image_paths if img['class'] == class_name)
    print(f"   {class_name:<12}: {count:>4} images")

# Save metadata
metadata = {
    'total_images': len(all_image_paths),
    'classes': config.ALL_CLASSES,
    'image_size': config.IMG_SIZE,
    'created_at': datetime.now().isoformat()
}

with open(os.path.join(config.RESULTS_DIR, 'dataset_metadata.json'), 'w') as f:
    json.dump(metadata, f, indent=2)

# ============================================================================
# CELL 4: Load ResNet50 with Preprocessing Layer
# ============================================================================

print("üéØ Loading ResNet50 model...")

base_model = ResNet50(weights='imagenet', input_shape=(config.IMG_SIZE, config.IMG_SIZE, 3))

print(f"‚úÖ ResNet50 loaded (parameters: {base_model.count_params():,})")

# Create model with preprocessing
input_layer = keras.layers.Input(shape=(config.IMG_SIZE, config.IMG_SIZE, 3))
x = keras.layers.Lambda(lambda x: x * 255.0)(input_layer)
x = keras.layers.Lambda(lambda x: preprocess_input(x))(x)
output = base_model(x)

model_with_preprocessing = keras.Model(inputs=input_layer, outputs=output)

print(f"‚úÖ Model with preprocessing created!")

# Wrap with ART
print(f"\nüîß Creating ART classifier...")

classifier = TensorFlowV2Classifier(
    model=model_with_preprocessing,
    nb_classes=1000,
    input_shape=(config.IMG_SIZE, config.IMG_SIZE, 3),
    loss_object=tf.keras.losses.CategoricalCrossentropy(),
    clip_values=(0.0, 1.0)
)

print(f"‚úÖ ART classifier ready!")

# ============================================================================
# CELL 5: Generate Adversarial Images
# ============================================================================

def generate_adversarial_dataset_pgd(pgd_config, classifier, image_paths, config,
                                      batch_size=32, visualize=True):
    """
    Generate adversarial images using PGD attack for given config

    Args:
        pgd_config: Dict with 'eps', 'eps_step', 'max_iter'
        classifier: ART classifier
        image_paths: List of image metadata dicts
        config: Configuration object
        batch_size: Batch size for processing
        visualize: Whether to create visualizations

    Returns:
        Dictionary with results and statistics
    """

    epsilon = pgd_config['eps']
    eps_step = pgd_config['eps_step']
    max_iter = pgd_config['max_iter']

    print("\n" + "="*70)
    print(f"‚öîÔ∏è  GENERATING PGD ADVERSARIAL DATASET")
    print(f"   Œµ = {epsilon}, step = {eps_step}, iterations = {max_iter}")
    print("="*70)

    # Create output directory structure
    epsilon_dir = os.path.join(config.ADVERSARIAL_DIR, f'eps_{epsilon:.3f}')

    for class_name in config.ALL_CLASSES:
        class_dir = os.path.join(epsilon_dir, class_name)
        os.makedirs(class_dir, exist_ok=True)

    print(f"\nüìÅ Output directory: {epsilon_dir}")

    # Create PGD attack
    print(f"\nüîß Creating PGD attack...")
    pgd = ProjectedGradientDescent(
        estimator=classifier,
        eps=epsilon,
        eps_step=eps_step,
        max_iter=max_iter,
        targeted=False,
        num_random_init=0,
        batch_size=batch_size,
        verbose=False
    )

    # Process images in batches
    print(f"\nüìä Processing {len(image_paths)} images in batches of {batch_size}...")

    attack_results = []
    all_originals = []
    all_adversarials = []

    # Process by class
    for class_name in config.ALL_CLASSES:
        print(f"\n   üìÇ Processing class: {class_name}")

        class_images = [img for img in image_paths if img['class'] == class_name]

        n_batches = (len(class_images) + batch_size - 1) // batch_size

        for batch_idx in tqdm(range(n_batches), desc=f"   {class_name}"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(class_images))
            batch_info = class_images[start_idx:end_idx]

            # Load batch
            batch_images = []
            for img_info in batch_info:
                try:
                    img = Image.open(img_info['path']).convert('RGB')
                    img = img.resize((config.IMG_SIZE, config.IMG_SIZE))
                    img_array = np.array(img).astype(np.float32) / 255.0
                    batch_images.append(img_array)
                except Exception as e:
                    print(f"\n      ‚ö†Ô∏è  Error loading {img_info['filename']}: {e}")
                    continue

            if len(batch_images) == 0:
                continue

            batch_images = np.array(batch_images)

            # Get original predictions
            orig_predictions = classifier.predict(batch_images)
            orig_classes = np.argmax(orig_predictions, axis=1)

            # Generate adversarial examples with PGD
            adv_images = pgd.generate(x=batch_images)
            adv_images = np.clip(adv_images, 0.0, 1.0)

            # Get adversarial predictions
            adv_predictions = classifier.predict(adv_images)
            adv_classes = np.argmax(adv_predictions, axis=1)

            # Calculate statistics
            attack_success = (orig_classes != adv_classes)
            perturbations = adv_images - batch_images
            l2_norms = np.linalg.norm(perturbations.reshape(len(perturbations), -1), ord=2, axis=1)
            linf_norms = np.max(np.abs(perturbations.reshape(len(perturbations), -1)), axis=1)

            # Save images and record results
            for i, img_info in enumerate(batch_info[:len(batch_images)]):
                # Save adversarial image
                adv_filename = img_info['filename']
                adv_path = os.path.join(epsilon_dir, class_name, adv_filename)

                adv_img_uint8 = (adv_images[i] * 255.0).astype(np.uint8)
                Image.fromarray(adv_img_uint8).save(adv_path)

                # Decode predictions
                orig_pred = decode_predictions(orig_predictions[i:i+1], top=1)[0][0]
                adv_pred = decode_predictions(adv_predictions[i:i+1], top=1)[0][0]

                # Record result
                attack_results.append({
                    'epsilon': epsilon,
                    'eps_step': eps_step,
                    'max_iter': max_iter,
                    'class': class_name,
                    'filename': img_info['filename'],
                    'original_path': img_info['path'],
                    'adversarial_path': adv_path,
                    'original_label': orig_pred[1],
                    'original_confidence': float(orig_pred[2]),
                    'adversarial_label': adv_pred[1],
                    'adversarial_confidence': float(adv_pred[2]),
                    'attack_success': bool(attack_success[i]),
                    'l2_norm': float(l2_norms[i]),
                    'linf_norm': float(linf_norms[i])
                })

            # Store examples for visualization
            if visualize and len(all_originals) < 40:
                for i in range(min(5, len(batch_images))):
                    all_originals.append(batch_images[i])
                    all_adversarials.append(adv_images[i])

    # Calculate statistics
    results_df = pd.DataFrame(attack_results)

    success_rate = results_df['attack_success'].mean() * 100
    avg_l2 = results_df['l2_norm'].mean()
    avg_linf = results_df['linf_norm'].mean()
    avg_conf_drop = (results_df['original_confidence'] - results_df['adversarial_confidence']).mean() * 100

    print(f"\n{'='*70}")
    print(f"‚úÖ PGD GENERATION COMPLETE FOR Œµ = {epsilon}")
    print(f"{'='*70}")
    print(f"   Total images: {len(results_df)}")
    print(f"   Success rate: {success_rate:.1f}%")
    print(f"   Avg L2 norm: {avg_l2:.4f}")
    print(f"   Avg L‚àû norm: {avg_linf:.4f}")
    print(f"   Avg confidence drop: {avg_conf_drop:.1f}%")

    # Save results
    results_file = os.path.join(config.RESULTS_DIR, f'results_pgd_eps_{epsilon:.3f}.csv')
    results_df.to_csv(results_file, index=False)
    print(f"\nüíæ Results saved: {results_file}")

    # Create visualizations
    if visualize and len(all_originals) > 0:
        print(f"\nüìä Creating visualizations...")
        create_visualizations(epsilon, np.array(all_originals[:40]),
                            np.array(all_adversarials[:40]),
                            results_df, config)

    # Return summary
    summary = {
        'epsilon': epsilon,
        'eps_step': eps_step,
        'max_iter': max_iter,
        'total_images': len(results_df),
        'success_rate': success_rate,
        'avg_l2': avg_l2,
        'avg_linf': avg_linf,
        'avg_confidence_drop': avg_conf_drop,
        'results_file': results_file,
        'adversarial_dir': epsilon_dir
    }

    return summary

print("‚úÖ Function defined: generate_adversarial_dataset_pgd()")

# ============================================================================
# CELL 6: Create Visualizations for Each Epsilon
# ============================================================================

def create_visualizations(epsilon, orig_images, adv_images, results_df, config):
    """Create all visualizations for given epsilon"""

    epsilon_plots_dir = os.path.join(config.PLOTS_DIR, f'eps_{epsilon:.3f}')
    os.makedirs(epsilon_plots_dir, exist_ok=True)

    # ========================================================================
    # 1. Comparison Grid (8 examples)
    # ========================================================================

    print(f"   Creating comparison grid...")

    n_examples = min(8, len(orig_images))
    indices = np.linspace(0, len(orig_images)-1, n_examples, dtype=int)

    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 6))

    for i, idx in enumerate(indices):
        # Original
        axes[0, i].imshow(orig_images[idx])
        axes[0, i].set_title('Original', fontsize=10)
        axes[0, i].axis('off')

        # Adversarial
        axes[1, i].imshow(adv_images[idx])
        axes[1, i].set_title('Adversarial', fontsize=10, color='red')
        axes[1, i].axis('off')

    plt.suptitle(f'Original vs Adversarial (Œµ={epsilon})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '01_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()

    # ========================================================================
    # 2. Perturbations
    # ========================================================================

    print(f"   Creating perturbation visualization...")

    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 6))

    for i, idx in enumerate(indices):
        # Adversarial
        axes[0, i].imshow(adv_images[idx])
        axes[0, i].set_title('Adversarial', fontsize=9)
        axes[0, i].axis('off')

        # Perturbation
        pert = adv_images[idx] - orig_images[idx]
        pert_vis = (pert * 10 + 0.5).clip(0, 1)
        axes[1, i].imshow(pert_vis)
        axes[1, i].set_title('Perturbation √ó10', fontsize=9)
        axes[1, i].axis('off')

    plt.suptitle(f'Perturbations (Œµ={epsilon})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '02_perturbations.png'), dpi=150, bbox_inches='tight')
    plt.close()

    # ========================================================================
    # 3. Statistical Analysis
    # ========================================================================

    print(f"   Creating statistical plots...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # Success rate by class
    class_success = results_df.groupby('class')['attack_success'].mean() * 100
    axes[0, 0].bar(class_success.index, class_success.values, color='#e74c3c', alpha=0.7)
    axes[0, 0].set_title('Attack Success Rate by Class', fontweight='bold')
    axes[0, 0].set_ylabel('Success Rate (%)')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(True, alpha=0.3, axis='y')

    # L2 norm distribution
    axes[0, 1].hist(results_df['l2_norm'], bins=50, color='#3498db', alpha=0.7)
    axes[0, 1].axvline(results_df['l2_norm'].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["l2_norm"].mean():.4f}')
    axes[0, 1].set_title('L2 Norm Distribution', fontweight='bold')
    axes[0, 1].set_xlabel('L2 Norm')
    axes[0, 1].set_ylabel('Count')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # L‚àû norm distribution
    axes[0, 2].hist(results_df['linf_norm'], bins=50, color='#2ecc71', alpha=0.7)
    axes[0, 2].axvline(results_df['linf_norm'].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["linf_norm"].mean():.4f}')
    axes[0, 2].set_title('L‚àû Norm Distribution', fontweight='bold')
    axes[0, 2].set_xlabel('L‚àû Norm')
    axes[0, 2].set_ylabel('Count')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Confidence change
    conf_change = results_df['original_confidence'] - results_df['adversarial_confidence']
    axes[1, 0].hist(conf_change * 100, bins=50, color='#9b59b6', alpha=0.7)
    axes[1, 0].axvline(conf_change.mean() * 100, color='red', linestyle='--',
                       label=f'Mean: {conf_change.mean()*100:.1f}%')
    axes[1, 0].set_title('Confidence Drop Distribution', fontweight='bold')
    axes[1, 0].set_xlabel('Confidence Drop (%)')
    axes[1, 0].set_ylabel('Count')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Success vs L2 scatter
    successful = results_df[results_df['attack_success']]
    failed = results_df[~results_df['attack_success']]

    axes[1, 1].scatter(successful['l2_norm'], successful['original_confidence'] - successful['adversarial_confidence'],
                      c='red', alpha=0.3, s=10, label='Success')
    if len(failed) > 0:
        axes[1, 1].scatter(failed['l2_norm'], failed['original_confidence'] - failed['adversarial_confidence'],
                          c='green', alpha=0.3, s=10, label='Failed')
    axes[1, 1].set_title('Perturbation vs Confidence Drop', fontweight='bold')
    axes[1, 1].set_xlabel('L2 Norm')
    axes[1, 1].set_ylabel('Confidence Drop')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # Images per class
    class_counts = results_df['class'].value_counts()
    axes[1, 2].barh(class_counts.index, class_counts.values, color='#34495e', alpha=0.7)
    axes[1, 2].set_title('Images per Class', fontweight='bold')
    axes[1, 2].set_xlabel('Count')
    axes[1, 2].grid(True, alpha=0.3, axis='x')

    plt.suptitle(f'Statistical Analysis (Œµ={epsilon})', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '03_statistics.png'), dpi=150, bbox_inches='tight')
    plt.close()

    print(f"   ‚úÖ Visualizations saved to: {epsilon_plots_dir}")

print("‚úÖ Function defined: create_visualizations()")

# ============================================================================
# CELL 7: Run Full Pipeline
# ============================================================================

print("\n" + "="*70)
print("üöÄ STARTING FULL PGD ADVERSARIAL GENERATION PIPELINE")
print("="*70)

all_summaries = []

for pgd_config in config.PGD_CONFIGS:
    summary = generate_adversarial_dataset_pgd(
        pgd_config=pgd_config,
        classifier=classifier,
        image_paths=all_image_paths,
        config=config,
        batch_size=32,
        visualize=True
    )
    all_summaries.append(summary)

# Final summary
print("\n" + "="*70)
print("üìä FINAL SUMMARY - ALL PGD CONFIGURATIONS")
print("="*70)

summary_df = pd.DataFrame(all_summaries)
print(summary_df[['epsilon', 'eps_step', 'max_iter', 'total_images', 'success_rate', 'avg_l2', 'avg_linf']].to_string(index=False))

# Save overall summary
summary_df.to_csv(os.path.join(config.RESULTS_DIR, 'pgd_all_results_summary.csv'), index=False)

print(f"\n‚úÖ ALL PGD ATTACKS COMPLETE!")
print(f"   Output directory: {config.BASE_DIR}")

# ============================================================================
# CELL 9: Package and Download
# ============================================================================

from google.colab import files

# Update README for PGD
readme_content = f"""# PGD Adversarial Dataset

- **Dataset**: Natural Images (Kaggle)
- **Attack Method**: PGD (Projected Gradient Descent)
- **Target Model**: ResNet50 (ImageNet pretrained)
- **Total Images**: {len(all_image_paths)}
- **Classes**: {', '.join(config.ALL_CLASSES)}

## PGD Configurations
| Epsilon | Step Size | Iterations |
|---------|-----------|------------|
| 0.02    | 0.002     | 10         |
| 0.05    | 0.005     | 20         |
| 0.08    | 0.004     | 30         |
| 0.10    | 0.005     | 40         |

## Directory Structure
```
adversarial_images/
‚îú‚îÄ‚îÄ eps_0.020/
‚îú‚îÄ‚îÄ eps_0.050/
‚îú‚îÄ‚îÄ eps_0.080/
‚îî‚îÄ‚îÄ eps_0.100/
```

Generated with Adversarial Robustness Toolbox (ART)
"""

readme_path = os.path.join(config.BASE_DIR, 'README.md')
with open(readme_path, 'w') as f:
    f.write(readme_content)

print(f"‚úÖ README created")

# Create ZIP archive
print(f"\nüóúÔ∏è  Creating ZIP archive...")
zip_name = f'adversarial_pgd_natural_images_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
zip_path = f'/content/{zip_name}'

shutil.make_archive(zip_path, 'zip', config.BASE_DIR)

zip_file = f'{zip_path}.zip'
zip_size = os.path.getsize(zip_file) / (1024 * 1024)

print(f"\n‚úÖ ZIP created: {zip_size:.1f} MB")

# Download
files.download(zip_file)
print(f"\n‚úÖ DOWNLOAD COMPLETE!")