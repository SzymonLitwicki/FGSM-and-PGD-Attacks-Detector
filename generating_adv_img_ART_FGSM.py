# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11IrbfL9jhJitOshSP8v32o2jO9DEauL1
"""

# ============================================================================
# CELL 1: Installation and Setup
# ============================================================================

!pip install -q adversarial-robustness-toolbox

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import os
import json
import shutil
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions

from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import TensorFlowV2Classifier

import pandas as pd

print("‚úÖ All packages loaded!")
print(f"   TensorFlow version: {tf.__version__}")
print(f"   GPU available: {tf.config.list_physical_devices('GPU')}")

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# ============================================================================
# CELL 2: Configuration for FULL Dataset
# ============================================================================

class Config:
    # Dataset
    DATASET_PATH = '/root/.cache/kagglehub/datasets/prasunroy/natural-images/versions/1/natural_images'

    # All classes
    ALL_CLASSES = ['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']

    # Load ALL images (not limited to 20)
    IMAGES_PER_CLASS = None  # None = load all images
    IMG_SIZE = 224

    # Working directories
    BASE_DIR = '/content/adversarial_full_dataset'
    ADVERSARIAL_DIR = os.path.join(BASE_DIR, 'adversarial_images')
    RESULTS_DIR = os.path.join(BASE_DIR, 'results')
    PLOTS_DIR = os.path.join(BASE_DIR, 'plots')

    RANDOM_SEED = 42

config = Config()

# Create directories
for directory in [config.BASE_DIR, config.ADVERSARIAL_DIR, config.RESULTS_DIR, config.PLOTS_DIR]:
    os.makedirs(directory, exist_ok=True)

print("‚öôÔ∏è  Configuration:")
print(f"   Dataset: Natural Images (FULL)")
print(f"   Classes: {config.ALL_CLASSES}")
print(f"   Images per class: ALL (no limit)")
print(f"   Image size: {config.IMG_SIZE}x{config.IMG_SIZE}")
print(f"   Output directory: {config.BASE_DIR}")
print("\n‚úÖ Directories created!")

# ============================================================================
# CELL 3: Download and Load FULL Natural Images Dataset
# ============================================================================

import kagglehub

print("üì• Downloading Natural Images dataset...")
dataset_path = kagglehub.dataset_download("prasunroy/natural-images")

if os.path.exists(os.path.join(dataset_path, 'natural_images')):
    config.DATASET_PATH = os.path.join(dataset_path, 'natural_images')
else:
    config.DATASET_PATH = dataset_path

print(f"‚úÖ Dataset path: {config.DATASET_PATH}")

# ============================================================================
# Load ALL images from dataset
# ============================================================================

print("\n" + "="*70)
print("üìä LOADING FULL DATASET")
print("="*70)

all_image_paths = []  # Store paths, not arrays (saves memory)

for class_idx, class_name in enumerate(config.ALL_CLASSES):
    class_path = os.path.join(config.DATASET_PATH, class_name)

    if not os.path.exists(class_path):
        print(f"   ‚ùå Class not found: {class_name}")
        continue

    # Get ALL image files
    all_files = [f for f in os.listdir(class_path)
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    print(f"   üìÇ {class_name}: Found {len(all_files)} images")

    for img_file in all_files:
        all_image_paths.append({
            'class': class_name,
            'class_idx': class_idx,
            'filename': img_file,
            'path': os.path.join(class_path, img_file)
        })

print(f"\n‚úÖ Dataset indexed!")
print(f"   Total images: {len(all_image_paths)}")

# Show distribution
print(f"\nüìä Class distribution:")
for class_name in config.ALL_CLASSES:
    count = sum(1 for img in all_image_paths if img['class'] == class_name)
    print(f"   {class_name:<12}: {count:>4} images")

# Save metadata
metadata = {
    'total_images': len(all_image_paths),
    'classes': config.ALL_CLASSES,
    'image_size': config.IMG_SIZE,
    'created_at': datetime.now().isoformat()
}

with open(os.path.join(config.RESULTS_DIR, 'dataset_metadata.json'), 'w') as f:
    json.dump(metadata, f, indent=2)

# ============================================================================
# CELL 4: Load ResNet50 with Preprocessing Layer
# ============================================================================

print("üéØ Loading ResNet50 model...")

base_model = ResNet50(weights='imagenet', input_shape=(config.IMG_SIZE, config.IMG_SIZE, 3))

print(f"‚úÖ ResNet50 loaded (parameters: {base_model.count_params():,})")

# Create model with preprocessing
input_layer = keras.layers.Input(shape=(config.IMG_SIZE, config.IMG_SIZE, 3))
x = keras.layers.Lambda(lambda x: x * 255.0)(input_layer)
x = keras.layers.Lambda(lambda x: preprocess_input(x))(x)
output = base_model(x)

model_with_preprocessing = keras.Model(inputs=input_layer, outputs=output)

print(f"‚úÖ Model with preprocessing created!")

# Wrap with ART
print(f"\nüîß Creating ART classifier...")

classifier = TensorFlowV2Classifier(
    model=model_with_preprocessing,
    nb_classes=1000,
    input_shape=(config.IMG_SIZE, config.IMG_SIZE, 3),
    loss_object=tf.keras.losses.CategoricalCrossentropy(),
    clip_values=(0.0, 1.0)
)

print(f"‚úÖ ART classifier ready!")

# ============================================================================
# CELL 5: Generate Adversarial Images for Entire Dataset (Single Epsilon)
# ============================================================================

def generate_adversarial_dataset(epsilon, classifier, image_paths, config,
                                  batch_size=32, visualize=True):
    """
    Generate adversarial images for entire dataset for given epsilon

    Args:
        epsilon: Attack strength
        classifier: ART classifier
        image_paths: List of image metadata dicts
        config: Configuration object
        batch_size: Batch size for processing
        visualize: Whether to create visualizations

    Returns:
        Dictionary with results and statistics
    """

    print("\n" + "="*70)
    print(f"‚öîÔ∏è  GENERATING ADVERSARIAL DATASET FOR Œµ = {epsilon}")
    print("="*70)

    # Create output directory structure
    epsilon_dir = os.path.join(config.ADVERSARIAL_DIR, f'eps_{epsilon:.3f}')

    for class_name in config.ALL_CLASSES:
        class_dir = os.path.join(epsilon_dir, class_name)
        os.makedirs(class_dir, exist_ok=True)

    print(f"\nüìÅ Output directory: {epsilon_dir}")

    # Create FGSM attack
    print(f"\nüîß Creating FGSM attack (Œµ={epsilon})...")
    fgsm = FastGradientMethod(
        estimator=classifier,
        eps=epsilon,
        eps_step=epsilon,
        targeted=False,
        num_random_init=0,
        batch_size=batch_size,
        minimal=False
    )

    # Process images in batches
    print(f"\nüìä Processing {len(image_paths)} images in batches of {batch_size}...")

    attack_results = []
    all_originals = []
    all_adversarials = []

    # Process by class for better organization
    for class_name in config.ALL_CLASSES:
        print(f"\n   üìÇ Processing class: {class_name}")

        # Get images for this class
        class_images = [img for img in image_paths if img['class'] == class_name]

        # Load images in batches
        n_batches = (len(class_images) + batch_size - 1) // batch_size

        for batch_idx in tqdm(range(n_batches), desc=f"   {class_name}"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(class_images))
            batch_info = class_images[start_idx:end_idx]

            # Load batch
            batch_images = []
            for img_info in batch_info:
                try:
                    img = Image.open(img_info['path']).convert('RGB')
                    img = img.resize((config.IMG_SIZE, config.IMG_SIZE))
                    img_array = np.array(img).astype(np.float32) / 255.0
                    batch_images.append(img_array)
                except Exception as e:
                    print(f"\n      ‚ö†Ô∏è  Error loading {img_info['filename']}: {e}")
                    continue

            if len(batch_images) == 0:
                continue

            batch_images = np.array(batch_images)

            # Get original predictions
            orig_predictions = classifier.predict(batch_images)
            orig_classes = np.argmax(orig_predictions, axis=1)

            # Generate adversarial examples
            adv_images = fgsm.generate(x=batch_images)
            adv_images = np.clip(adv_images, 0.0, 1.0)

            # Get adversarial predictions
            adv_predictions = classifier.predict(adv_images)
            adv_classes = np.argmax(adv_predictions, axis=1)

            # Calculate statistics
            attack_success = (orig_classes != adv_classes)
            perturbations = adv_images - batch_images
            l2_norms = np.linalg.norm(perturbations.reshape(len(perturbations), -1), ord=2, axis=1)
            linf_norms = np.max(np.abs(perturbations.reshape(len(perturbations), -1)), axis=1)

            # Save images and record results
            for i, img_info in enumerate(batch_info[:len(batch_images)]):
                # Save adversarial image
                adv_filename = img_info['filename']
                adv_path = os.path.join(epsilon_dir, class_name, adv_filename)

                adv_img_uint8 = (adv_images[i] * 255.0).astype(np.uint8)
                Image.fromarray(adv_img_uint8).save(adv_path)

                # Decode predictions
                orig_pred = decode_predictions(orig_predictions[i:i+1], top=1)[0][0]
                adv_pred = decode_predictions(adv_predictions[i:i+1], top=1)[0][0]

                # Record result
                attack_results.append({
                    'epsilon': epsilon,
                    'class': class_name,
                    'filename': img_info['filename'],
                    'original_path': img_info['path'],
                    'adversarial_path': adv_path,
                    'original_label': orig_pred[1],
                    'original_confidence': float(orig_pred[2]),
                    'adversarial_label': adv_pred[1],
                    'adversarial_confidence': float(adv_pred[2]),
                    'attack_success': bool(attack_success[i]),
                    'l2_norm': float(l2_norms[i]),
                    'linf_norm': float(linf_norms[i])
                })

            # Store some examples for visualization
            if visualize and len(all_originals) < 40:  # Keep 40 examples
                for i in range(min(5, len(batch_images))):  # 5 per batch
                    all_originals.append(batch_images[i])
                    all_adversarials.append(adv_images[i])

    # Calculate statistics
    results_df = pd.DataFrame(attack_results)

    success_rate = results_df['attack_success'].mean() * 100
    avg_l2 = results_df['l2_norm'].mean()
    avg_linf = results_df['linf_norm'].mean()
    avg_conf_drop = (results_df['original_confidence'] - results_df['adversarial_confidence']).mean() * 100

    print(f"\n{'='*70}")
    print(f"‚úÖ ADVERSARIAL GENERATION COMPLETE FOR Œµ = {epsilon}")
    print(f"{'='*70}")
    print(f"   Total images: {len(results_df)}")
    print(f"   Success rate: {success_rate:.1f}%")
    print(f"   Avg L2 norm: {avg_l2:.4f}")
    print(f"   Avg L‚àû norm: {avg_linf:.4f}")
    print(f"   Avg confidence drop: {avg_conf_drop:.1f}%")

    # Save results
    results_file = os.path.join(config.RESULTS_DIR, f'results_eps_{epsilon:.3f}.csv')
    results_df.to_csv(results_file, index=False)
    print(f"\nüíæ Results saved: {results_file}")

    # Create visualizations
    if visualize and len(all_originals) > 0:
        print(f"\nüìä Creating visualizations...")
        create_visualizations(epsilon, np.array(all_originals[:40]),
                            np.array(all_adversarials[:40]),
                            results_df, config)

    # Return summary
    summary = {
        'epsilon': epsilon,
        'total_images': len(results_df),
        'success_rate': success_rate,
        'avg_l2': avg_l2,
        'avg_linf': avg_linf,
        'avg_confidence_drop': avg_conf_drop,
        'results_file': results_file,
        'adversarial_dir': epsilon_dir
    }

    return summary

print("‚úÖ Function defined: generate_adversarial_dataset()")

# ============================================================================
# CELL 6: Create Visualizations for Each Epsilon
# ============================================================================

def create_visualizations(epsilon, orig_images, adv_images, results_df, config):
    """Create all visualizations for given epsilon"""

    epsilon_plots_dir = os.path.join(config.PLOTS_DIR, f'eps_{epsilon:.3f}')
    os.makedirs(epsilon_plots_dir, exist_ok=True)

    # ========================================================================
    # 1. Comparison Grid (8 examples)
    # ========================================================================

    print(f"   Creating comparison grid...")

    n_examples = min(8, len(orig_images))
    indices = np.linspace(0, len(orig_images)-1, n_examples, dtype=int)

    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 6))

    for i, idx in enumerate(indices):
        # Original
        axes[0, i].imshow(orig_images[idx])
        axes[0, i].set_title('Original', fontsize=10)
        axes[0, i].axis('off')

        # Adversarial
        axes[1, i].imshow(adv_images[idx])
        axes[1, i].set_title('Adversarial', fontsize=10, color='red')
        axes[1, i].axis('off')

    plt.suptitle(f'Original vs Adversarial (Œµ={epsilon})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '01_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()

    # ========================================================================
    # 2. Perturbations
    # ========================================================================

    print(f"   Creating perturbation visualization...")

    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 6))

    for i, idx in enumerate(indices):
        # Adversarial
        axes[0, i].imshow(adv_images[idx])
        axes[0, i].set_title('Adversarial', fontsize=9)
        axes[0, i].axis('off')

        # Perturbation
        pert = adv_images[idx] - orig_images[idx]
        pert_vis = (pert * 10 + 0.5).clip(0, 1)
        axes[1, i].imshow(pert_vis)
        axes[1, i].set_title('Perturbation √ó10', fontsize=9)
        axes[1, i].axis('off')

    plt.suptitle(f'Perturbations (Œµ={epsilon})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '02_perturbations.png'), dpi=150, bbox_inches='tight')
    plt.close()

    # ========================================================================
    # 3. Statistical Analysis
    # ========================================================================

    print(f"   Creating statistical plots...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # Success rate by class
    class_success = results_df.groupby('class')['attack_success'].mean() * 100
    axes[0, 0].bar(class_success.index, class_success.values, color='#e74c3c', alpha=0.7)
    axes[0, 0].set_title('Attack Success Rate by Class', fontweight='bold')
    axes[0, 0].set_ylabel('Success Rate (%)')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(True, alpha=0.3, axis='y')

    # L2 norm distribution
    axes[0, 1].hist(results_df['l2_norm'], bins=50, color='#3498db', alpha=0.7)
    axes[0, 1].axvline(results_df['l2_norm'].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["l2_norm"].mean():.4f}')
    axes[0, 1].set_title('L2 Norm Distribution', fontweight='bold')
    axes[0, 1].set_xlabel('L2 Norm')
    axes[0, 1].set_ylabel('Count')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # L‚àû norm distribution
    axes[0, 2].hist(results_df['linf_norm'], bins=50, color='#2ecc71', alpha=0.7)
    axes[0, 2].axvline(results_df['linf_norm'].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["linf_norm"].mean():.4f}')
    axes[0, 2].set_title('L‚àû Norm Distribution', fontweight='bold')
    axes[0, 2].set_xlabel('L‚àû Norm')
    axes[0, 2].set_ylabel('Count')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Confidence change
    conf_change = results_df['original_confidence'] - results_df['adversarial_confidence']
    axes[1, 0].hist(conf_change * 100, bins=50, color='#9b59b6', alpha=0.7)
    axes[1, 0].axvline(conf_change.mean() * 100, color='red', linestyle='--',
                       label=f'Mean: {conf_change.mean()*100:.1f}%')
    axes[1, 0].set_title('Confidence Drop Distribution', fontweight='bold')
    axes[1, 0].set_xlabel('Confidence Drop (%)')
    axes[1, 0].set_ylabel('Count')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Success vs L2 scatter
    successful = results_df[results_df['attack_success']]
    failed = results_df[~results_df['attack_success']]

    axes[1, 1].scatter(successful['l2_norm'], successful['original_confidence'] - successful['adversarial_confidence'],
                      c='red', alpha=0.3, s=10, label='Success')
    if len(failed) > 0:
        axes[1, 1].scatter(failed['l2_norm'], failed['original_confidence'] - failed['adversarial_confidence'],
                          c='green', alpha=0.3, s=10, label='Failed')
    axes[1, 1].set_title('Perturbation vs Confidence Drop', fontweight='bold')
    axes[1, 1].set_xlabel('L2 Norm')
    axes[1, 1].set_ylabel('Confidence Drop')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # Images per class
    class_counts = results_df['class'].value_counts()
    axes[1, 2].barh(class_counts.index, class_counts.values, color='#34495e', alpha=0.7)
    axes[1, 2].set_title('Images per Class', fontweight='bold')
    axes[1, 2].set_xlabel('Count')
    axes[1, 2].grid(True, alpha=0.3, axis='x')

    plt.suptitle(f'Statistical Analysis (Œµ={epsilon})', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(epsilon_plots_dir, '03_statistics.png'), dpi=150, bbox_inches='tight')
    plt.close()

    print(f"   ‚úÖ Visualizations saved to: {epsilon_plots_dir}")

print("‚úÖ Function defined: create_visualizations()")

# ============================================================================
# CELL 7: Generate Adversarial Dataset for Single Epsilon
# ============================================================================

# Choose epsilon value
EPSILON = 0.1  # Change this value as needed: 0.01, 0.02, 0.05, 0.08, 0.10

# Generate adversarial dataset
summary = generate_adversarial_dataset(
    epsilon=EPSILON,
    classifier=classifier,
    image_paths=all_image_paths,
    config=config,
    batch_size=32,
    visualize=True
)

# Print summary
print("\n" + "="*70)
print("üìä SUMMARY")
print("="*70)
print(f"   Epsilon: {summary['epsilon']}")
print(f"   Total images: {summary['total_images']}")
print(f"   Success rate: {summary['success_rate']:.1f}%")
print(f"   Avg L2: {summary['avg_l2']:.4f}")
print(f"   Avg L‚àû: {summary['avg_linf']:.4f}")
print(f"   Results: {summary['results_file']}")
print(f"   Images: {summary['adversarial_dir']}")
print("="*70)

# ============================================================================
# CELL 9: Package Everything and Download
# ============================================================================

from google.colab import files
import shutil

print("üì¶ PACKAGING ADVERSARIAL DATASET")
print("="*70)

# Check directory size
def get_dir_size(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.exists(fp):
                total += os.path.getsize(fp)
    return total / (1024 * 1024)  # MB

print(f"\nüìä Directory sizes:")
print(f"   Adversarial images: {get_dir_size(config.ADVERSARIAL_DIR):.1f} MB")
print(f"   Results: {get_dir_size(config.RESULTS_DIR):.1f} MB")
print(f"   Plots: {get_dir_size(config.PLOTS_DIR):.1f} MB")
print(f"   Total: {get_dir_size(config.BASE_DIR):.1f} MB")

# Create README
readme_content = f"""# Adversarial Images Dataset - Natural Images

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Dataset Information

- **Original Dataset**: Natural Images (Kaggle)
- **Attack Method**: FGSM (Fast Gradient Sign Method)
- **Target Model**: ResNet50 (ImageNet pretrained)
- **Total Images**: {len(all_image_paths)}
- **Classes**: {', '.join(config.ALL_CLASSES)}

## Directory Structure
```
adversarial_images/
‚îú‚îÄ‚îÄ eps_0.010/
‚îÇ   ‚îú‚îÄ‚îÄ airplane/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ car/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ eps_0.020/
‚îî‚îÄ‚îÄ ...

results/
‚îú‚îÄ‚îÄ results_eps_0.010.csv
‚îú‚îÄ‚îÄ results_eps_0.020.csv
‚îî‚îÄ‚îÄ ...

plots/
‚îú‚îÄ‚îÄ eps_0.010/
‚îÇ   ‚îú‚îÄ‚îÄ 01_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ 02_perturbations.png
‚îÇ   ‚îî‚îÄ‚îÄ 03_statistics.png
‚îî‚îÄ‚îÄ ...
```

## Results Files

Each `results_eps_X.csv` contains:
- filename
- class
- original_label, original_confidence
- adversarial_label, adversarial_confidence
- attack_success (boolean)
- l2_norm, linf_norm

## Usage
```python
# Load adversarial image
from PIL import Image
img = Image.open('adversarial_images/eps_0.050/airplane/img001.jpg')

# Load results
import pandas as pd
results = pd.read_csv('results/results_eps_0.050.csv')
```

## Citation

If you use this dataset, please cite:
- Natural Images Dataset: https://www.kaggle.com/datasets/prasunroy/natural-images
- Adversarial Robustness Toolbox (ART)
- ResNet50: He et al., "Deep Residual Learning for Image Recognition"

Generated with Adversarial Robustness Toolbox (ART)
"""

readme_path = os.path.join(config.BASE_DIR, 'README.md')
with open(readme_path, 'w') as f:
    f.write(readme_content)

print(f"\n‚úÖ README created")

# Create ZIP archive
print(f"\nüóúÔ∏è  Creating ZIP archive...")
print(f"   This may take several minutes for large datasets...")

zip_name = f'adversarial_natural_images_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
zip_path = f'/content/{zip_name}'

shutil.make_archive(zip_path, 'zip', config.BASE_DIR)

zip_file = f'{zip_path}.zip'
zip_size = os.path.getsize(zip_file) / (1024 * 1024)

print(f"\n‚úÖ ZIP created!")
print(f"   File: {zip_file}")
print(f"   Size: {zip_size:.1f} MB")

# Download
print(f"\n‚¨áÔ∏è  Downloading...")
files.download(zip_file)

print(f"\n{'='*70}")
print("‚úÖ DOWNLOAD COMPLETE!")
print(f"{'='*70}")
print(f"\nüì• Check your downloads folder for: {zip_name}.zip")
print(f"\nüìÇ Contents:")
print(f"   ‚Ä¢ adversarial_images/ - Organized by epsilon and class")
print(f"   ‚Ä¢ results/ - CSV files with attack statistics")
print(f"   ‚Ä¢ plots/ - Visualizations for each epsilon")
print(f"   ‚Ä¢ README.md - Documentation")